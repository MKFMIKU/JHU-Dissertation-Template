\noaddvspace 
\babel@toc {american}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Text-to-image results from our scaled LDMs (\texttt {39M} - \texttt {2B}), highlighting the improvement in visual quality with increased model size (note: 39M model is the exception). All images generated using 50-step DDIM sampling and CFG rate of 7.5. \relax }}{6}{figure.caption.6}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Our scaled latent diffusion models vary in the number of filters within the denoising U-Net. Other modules remain consistent. Smooth channel scaling (64 to 768) within residual blocks yields models ranging from \texttt {39M} to \texttt {5B} parameters. For downstream tasks requiring image input, we use an encoder to generate a latent code; this code is then concatenated with the noise vector in the denoising U-Net.\relax }}{10}{figure.caption.8}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces In text-to-image generation using 50-step DDIM sampling and CFG rate of 7.5, we observe consistent trends across various model sizes in how quality metrics (FID and CLIP scores) relate to training compute (\emph {i.e}\onedot , the total GFLOPS spend on training). Under moderate training resources, training compute is the most relevant factor dominating quality. \relax }}{10}{figure.caption.9}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces In $4\times $ real image super-resolution using 50-step DDIM sampling, FID and LPIPS scores reveal an interesting divergence. Model size drives FID score improvement, while training compute most impacts LPIPS score. Despite this, visual assessment (Fig.~\ref {fig:sr}) confirms the importance of model size for superior detail recovery (similarly as observed in the text-to-image pretraining). \relax }}{11}{figure.caption.10}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces In 4$\times $ super-resolution using 50-step DDIM sampling, visual quality directly improves with increased model size. As these scaled models vary in pretraining performance, the results clearly demonstrate that pretraining boosts super-resolution capabilities in both quantitative (Fig~\ref {fig:sr_compute}) and qualitative ways. \relax }}{12}{figure.caption.11}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Visualization of the Dreambooth results (using 50-step DDIM sampling and CFG rate of 7.5) shows two distinct tiers based on model size. Smaller models (\texttt {83M}-\texttt {223M}) perform similarly, as do larger ones (\texttt {318M}-\texttt {2B}), with a clear quality advantage for the larger group. \relax }}{13}{figure.caption.12}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Visualization of text-to-image results with 50-step DDIM sampling and different CFG rates (from left to right in each row: $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$). The prompt used is ``\emph {A raccoon wearing formal clothes, wearing a top hat and holding a cane. Oil painting in the style of Rembrandt.}''. We observe that changes in CFG rates impact visual quality more significantly than the prompt semantic accuracy. We use the FID score for quantitative determination of optimal sampling performance (Fig.~\ref {fig:cfgrate}) because it directly measures visual quality, unlike the CLIP score, which focuses on semantic similarity. \relax }}{14}{figure.caption.13}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces The impact of the CFG rate on text-to-image generation depends on the model size and sampling steps. As demonstrated in the left and center panels, the optimal CFG rate changes as the sampling steps increased. To determine the optimal performance (according to the FID score) of each model and each sampling steps, we systematically sample the model at various CFG rates and identify the best one. As a reference of the optimal performance, the right panel shows the CFG rate corresponding to the optimal performance of each model for a given number of sampling steps. \relax }}{15}{figure.caption.14}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Comparison of text-to-image performance of models with varying sizes. The left figure shows the relationship between sampling cost (normalized cost $\times $ sampling steps) and sampling steps for different model sizes. The right figure plots the optimal text-to-image FID score among CFG rates of $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$ as a function of the sampling cost for the same models. Key Observation: Smaller models achieve better FID scores than larger models for a fixed sampling cost. For instance, at a cost of 3, the 83M model achieves the best FID compared to the larger models. This suggests that smaller models can be more efficient in achieving good results with lower costs. \relax }}{16}{figure.caption.15}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Text-to-image results of the scaled LDMs under approximately the same inference cost (normalized cost $\times $ sampling steps). Smaller models can produce comparable or even better visual results than larger models under similar sampling cost. \relax }}{17}{figure.caption.16}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces \emph {Left}: Text-to-image performance FID as a function of the sampling cost (normalized cost $\times $ sampling steps) for the DDPM sampler (solid curves) and the DDIM sampler (dashed curves). \emph {Right}: Text-to-image performance FID as a function of the sampling cost for the second-order DPM-Solver++ sampler (solid curves) and the DDIM sampler (dashed curves). Suggested by the trends shown in Fig.~\ref {fig:optiamlrules}, we only show the sampling steps $\leq 50$ as using more steps does not improve the performance.\relax }}{18}{figure.caption.17}%
\contentsline {figure}{\numberline {2.12}{\ignorespaces Super-resolution performance vs. sampling cost for different model sizes. \emph {Left:} FID scores of super-resolution models under limited sampling steps (less than or equal to 20). Smaller models tend to achieve lower (better) FID scores within this range. \emph {Right:} FID scores of super-resolution models under a larger number of sampling steps (greater than 20). Performance differences between models become less pronounced as sampling steps increase. \relax }}{19}{figure.caption.18}%
\contentsline {figure}{\numberline {2.13}{\ignorespaces Distillation improves text-to-image performance and scalability. \emph {Left:} Distilled Latent Diffusion Models (LDMs) consistently exhibit lower (better) FID scores compared to their undistilled counterparts across varying model sizes. The consistent acceleration factor (approx. $5\times $) indicates that the benefits of distillation scale well with model size. \emph {Right:} Distilled models using only 4 sampling steps achieve FID scores comparable to undistilled models using significantly more steps. Interestingly, at a sampling cost of 7, the distilled \texttt {866M} model performs similarly to the smaller, undistilled \texttt {83M} model, suggesting improved efficiency. \relax }}{20}{figure.caption.19}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Here are some photos of ducks to make you feel happy in tough times.\relax }}{26}{figure.caption.20}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
