\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Scaling Properties of Latent Diffusion Models}{4}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:chap-2}{{2}{4}{Scaling Properties of Latent Diffusion Models}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{4}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Text-to-image results from our scaled LDMs (\texttt  {39M} - \texttt  {2B}), highlighting the improvement in visual quality with increased model size (note: 39M model is the exception). All images generated using 50-step DDIM sampling and CFG rate of 7.5. \relax }}{6}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:t2i_results}{{2.1}{6}{Text-to-image results from our scaled LDMs (\texttt {39M} - \texttt {2B}), highlighting the improvement in visual quality with increased model size (note: 39M model is the exception). All images generated using 50-step DDIM sampling and CFG rate of 7.5. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Summary}{7}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Scaling LDMs}{8}{section.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces We scale the baseline LDM (\emph  {i.e}\onedot  , \texttt  {866M} Stable Diffusion v1.5) by changing the base number of channels $c$ that controls the rest of the U-Net architecture as $[c, 2c, 4c, 4c]$ (See Fig.~\ref {fig:scaling_arch}). GFLOPS are measured for an input latent of shape $64\times 64 \times 4$ with FP32. We also show a normalized running cost with respect to the baseline model. The text-to-image performance (FID and CLIP scores) for all scaled LDMs is evaluated on the COCO-2014 validation set with 30k samples, using 50-step DDIM sampling and Classifier-free Guidance (CFG) with a rate of 7.5. It is worth noting that all the model sizes, and the training and the inference costs reported in this work only refer to the denoising UNet in the latent space, and do not include the \texttt  {1.4B} text encoder and the \texttt  {250M} latent encoder and decoder. \relax }}{9}{table.caption.7}\protected@file@percent }
\newlabel{tab:scaling_config}{{2.1}{9}{We scale the baseline LDM (\ie , \texttt {866M} Stable Diffusion v1.5) by changing the base number of channels $c$ that controls the rest of the U-Net architecture as $[c, 2c, 4c, 4c]$ (See Fig.~\ref {fig:scaling_arch}). GFLOPS are measured for an input latent of shape $64\times 64 \times 4$ with FP32. We also show a normalized running cost with respect to the baseline model. The text-to-image performance (FID and CLIP scores) for all scaled LDMs is evaluated on the COCO-2014 validation set with 30k samples, using 50-step DDIM sampling and Classifier-free Guidance (CFG) with a rate of 7.5. It is worth noting that all the model sizes, and the training and the inference costs reported in this work only refer to the denoising UNet in the latent space, and do not include the \texttt {1.4B} text encoder and the \texttt {250M} latent encoder and decoder. \relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Training compute scales text-to-image performance}{9}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:scalingt2i}{{2.2.1}{9}{Training compute scales text-to-image performance}{subsection.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Our scaled latent diffusion models vary in the number of filters within the denoising U-Net. Other modules remain consistent. Smooth channel scaling (64 to 768) within residual blocks yields models ranging from \texttt  {39M} to \texttt  {5B} parameters. For downstream tasks requiring image input, we use an encoder to generate a latent code; this code is then concatenated with the noise vector in the denoising U-Net.\relax }}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:scaling_arch}{{2.2}{10}{Our scaled latent diffusion models vary in the number of filters within the denoising U-Net. Other modules remain consistent. Smooth channel scaling (64 to 768) within residual blocks yields models ranging from \texttt {39M} to \texttt {5B} parameters. For downstream tasks requiring image input, we use an encoder to generate a latent code; this code is then concatenated with the noise vector in the denoising U-Net.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces In text-to-image generation using 50-step DDIM sampling and CFG rate of 7.5, we observe consistent trends across various model sizes in how quality metrics (FID and CLIP scores) relate to training compute (\emph  {i.e}\onedot  , the total GFLOPS spend on training). Under moderate training resources, training compute is the most relevant factor dominating quality. \relax }}{10}{figure.caption.9}\protected@file@percent }
\newlabel{fig:t2i_compute}{{2.3}{10}{In text-to-image generation using 50-step DDIM sampling and CFG rate of 7.5, we observe consistent trends across various model sizes in how quality metrics (FID and CLIP scores) relate to training compute (\ie , the total GFLOPS spend on training). Under moderate training resources, training compute is the most relevant factor dominating quality. \relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces In $4\times $ real image super-resolution using 50-step DDIM sampling, FID and LPIPS scores reveal an interesting divergence. Model size drives FID score improvement, while training compute most impacts LPIPS score. Despite this, visual assessment (Fig.~\ref {fig:sr}) confirms the importance of model size for superior detail recovery (similarly as observed in the text-to-image pretraining). \relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sr_compute}{{2.4}{11}{In $4\times $ real image super-resolution using 50-step DDIM sampling, FID and LPIPS scores reveal an interesting divergence. Model size drives FID score improvement, while training compute most impacts LPIPS score. Despite this, visual assessment (Fig.~\ref {fig:sr}) confirms the importance of model size for superior detail recovery (similarly as observed in the text-to-image pretraining). \relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Pretraining scales downstream performance}{11}{subsection.2.2.2}\protected@file@percent }
\newlabel{sec:scalingsr}{{2.2.2}{11}{Pretraining scales downstream performance}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces In 4$\times $ super-resolution using 50-step DDIM sampling, visual quality directly improves with increased model size. As these scaled models vary in pretraining performance, the results clearly demonstrate that pretraining boosts super-resolution capabilities in both quantitative (Fig~\ref {fig:sr_compute}) and qualitative ways. \relax }}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:sr}{{2.5}{12}{In 4$\times $ super-resolution using 50-step DDIM sampling, visual quality directly improves with increased model size. As these scaled models vary in pretraining performance, the results clearly demonstrate that pretraining boosts super-resolution capabilities in both quantitative (Fig~\ref {fig:sr_compute}) and qualitative ways. \relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Visualization of the Dreambooth results (using 50-step DDIM sampling and CFG rate of 7.5) shows two distinct tiers based on model size. Smaller models (\texttt  {83M}-\texttt  {223M}) perform similarly, as do larger ones (\texttt  {318M}-\texttt  {2B}), with a clear quality advantage for the larger group. \relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:dreambooth}{{2.6}{13}{Visualization of the Dreambooth results (using 50-step DDIM sampling and CFG rate of 7.5) shows two distinct tiers based on model size. Smaller models (\texttt {83M}-\texttt {223M}) perform similarly, as do larger ones (\texttt {318M}-\texttt {2B}), with a clear quality advantage for the larger group. \relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Scaling sampling-efficiency}{13}{subsection.2.2.3}\protected@file@percent }
\newlabel{sec:scalesampleeff}{{2.2.3}{13}{Scaling sampling-efficiency}{subsection.2.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Analyzing the effect of CFG rate.}{13}{figure.caption.14}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Visualization of text-to-image results with 50-step DDIM sampling and different CFG rates (from left to right in each row: $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$). The prompt used is ``\emph  {A raccoon wearing formal clothes, wearing a top hat and holding a cane. Oil painting in the style of Rembrandt.}''. We observe that changes in CFG rates impact visual quality more significantly than the prompt semantic accuracy. We use the FID score for quantitative determination of optimal sampling performance (Fig.~\ref {fig:cfgrate}) because it directly measures visual quality, unlike the CLIP score, which focuses on semantic similarity. \relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:cfgratevisual}{{2.7}{14}{Visualization of text-to-image results with 50-step DDIM sampling and different CFG rates (from left to right in each row: $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$). The prompt used is ``\emph {A raccoon wearing formal clothes, wearing a top hat and holding a cane. Oil painting in the style of Rembrandt.}''. We observe that changes in CFG rates impact visual quality more significantly than the prompt semantic accuracy. We use the FID score for quantitative determination of optimal sampling performance (Fig.~\ref {fig:cfgrate}) because it directly measures visual quality, unlike the CLIP score, which focuses on semantic similarity. \relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces  The impact of the CFG rate on text-to-image generation depends on the model size and sampling steps. As demonstrated in the left and center panels, the optimal CFG rate changes as the sampling steps increased. To determine the optimal performance (according to the FID score) of each model and each sampling steps, we systematically sample the model at various CFG rates and identify the best one. As a reference of the optimal performance, the right panel shows the CFG rate corresponding to the optimal performance of each model for a given number of sampling steps. \relax }}{15}{figure.caption.14}\protected@file@percent }
\newlabel{fig:cfgrate}{{2.8}{15}{The impact of the CFG rate on text-to-image generation depends on the model size and sampling steps. As demonstrated in the left and center panels, the optimal CFG rate changes as the sampling steps increased. To determine the optimal performance (according to the FID score) of each model and each sampling steps, we systematically sample the model at various CFG rates and identify the best one. As a reference of the optimal performance, the right panel shows the CFG rate corresponding to the optimal performance of each model for a given number of sampling steps. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling efficiency trends.}{15}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Comparison of text-to-image performance of models with varying sizes. The left figure shows the relationship between sampling cost (normalized cost $\times $ sampling steps) and sampling steps for different model sizes. The right figure plots the optimal text-to-image FID score among CFG rates of $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$ as a function of the sampling cost for the same models. Key Observation: Smaller models achieve better FID scores than larger models for a fixed sampling cost. For instance, at a cost of 3, the 83M model achieves the best FID compared to the larger models. This suggests that smaller models can be more efficient in achieving good results with lower costs. \relax }}{16}{figure.caption.15}\protected@file@percent }
\newlabel{fig:optiamlrules}{{2.9}{16}{Comparison of text-to-image performance of models with varying sizes. The left figure shows the relationship between sampling cost (normalized cost $\times $ sampling steps) and sampling steps for different model sizes. The right figure plots the optimal text-to-image FID score among CFG rates of $(1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)$ as a function of the sampling cost for the same models. Key Observation: Smaller models achieve better FID scores than larger models for a fixed sampling cost. For instance, at a cost of 3, the 83M model achieves the best FID compared to the larger models. This suggests that smaller models can be more efficient in achieving good results with lower costs. \relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Scaling sampling-efficiency in different samplers}{16}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Text-to-image results of the scaled LDMs under approximately the same inference cost (normalized cost $\times $ sampling steps). Smaller models can produce comparable or even better visual results than larger models under similar sampling cost. \relax }}{17}{figure.caption.16}\protected@file@percent }
\newlabel{fig:optimalvisual}{{2.10}{17}{Text-to-image results of the scaled LDMs under approximately the same inference cost (normalized cost $\times $ sampling steps). Smaller models can produce comparable or even better visual results than larger models under similar sampling cost. \relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \emph  {Left}: Text-to-image performance FID as a function of the sampling cost (normalized cost $\times $ sampling steps) for the DDPM sampler (solid curves) and the DDIM sampler (dashed curves). \emph  {Right}: Text-to-image performance FID as a function of the sampling cost for the second-order DPM-Solver++ sampler (solid curves) and the DDIM sampler (dashed curves). Suggested by the trends shown in Fig.~\ref {fig:optiamlrules}, we only show the sampling steps $\leq 50$ as using more steps does not improve the performance.\relax }}{18}{figure.caption.17}\protected@file@percent }
\newlabel{fig:scalingsampler}{{2.11}{18}{\emph {Left}: Text-to-image performance FID as a function of the sampling cost (normalized cost $\times $ sampling steps) for the DDPM sampler (solid curves) and the DDIM sampler (dashed curves). \emph {Right}: Text-to-image performance FID as a function of the sampling cost for the second-order DPM-Solver++ sampler (solid curves) and the DDIM sampler (dashed curves). Suggested by the trends shown in Fig.~\ref {fig:optiamlrules}, we only show the sampling steps $\leq 50$ as using more steps does not improve the performance.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Scaling downstream sampling-efficiency}{18}{subsection.2.2.4}\protected@file@percent }
\newlabel{sec:scalingsamplingsr}{{2.2.4}{18}{Scaling downstream sampling-efficiency}{subsection.2.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Super-resolution performance vs. sampling cost for different model sizes. \emph  {Left:} FID scores of super-resolution models under limited sampling steps (less than or equal to 20). Smaller models tend to achieve lower (better) FID scores within this range. \emph  {Right:} FID scores of super-resolution models under a larger number of sampling steps (greater than 20). Performance differences between models become less pronounced as sampling steps increase. \relax }}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:sroptiamlrules}{{2.12}{19}{Super-resolution performance vs. sampling cost for different model sizes. \emph {Left:} FID scores of super-resolution models under limited sampling steps (less than or equal to 20). Smaller models tend to achieve lower (better) FID scores within this range. \emph {Right:} FID scores of super-resolution models under a larger number of sampling steps (greater than 20). Performance differences between models become less pronounced as sampling steps increase. \relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Scaling sampling-efficiency in distilled LDMs.}{19}{subsection.2.2.5}\protected@file@percent }
\newlabel{sec:scalingdistill}{{2.2.5}{19}{Scaling sampling-efficiency in distilled LDMs}{subsection.2.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Distillation improves text-to-image performance and scalability. \emph  {Left:} Distilled Latent Diffusion Models (LDMs) consistently exhibit lower (better) FID scores compared to their undistilled counterparts across varying model sizes. The consistent acceleration factor (approx. $5\times $) indicates that the benefits of distillation scale well with model size. \emph  {Right:} Distilled models using only 4 sampling steps achieve FID scores comparable to undistilled models using significantly more steps. Interestingly, at a sampling cost of 7, the distilled \texttt  {866M} model performs similarly to the smaller, undistilled \texttt  {83M} model, suggesting improved efficiency. \relax }}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:distll}{{2.13}{20}{Distillation improves text-to-image performance and scalability. \emph {Left:} Distilled Latent Diffusion Models (LDMs) consistently exhibit lower (better) FID scores compared to their undistilled counterparts across varying model sizes. The consistent acceleration factor (approx. $5\times $) indicates that the benefits of distillation scale well with model size. \emph {Right:} Distilled models using only 4 sampling steps achieve FID scores comparable to undistilled models using significantly more steps. Interestingly, at a sampling cost of 7, the distilled \texttt {866M} model performs similarly to the smaller, undistilled \texttt {83M} model, suggesting improved efficiency. \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Conclusion}{21}{section.2.3}\protected@file@percent }
\@setckpt{07-chapter-2}{
\setcounter{page}{22}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{13}
\setcounter{table}{1}
\setcounter{parentequation}{0}
\setcounter{@autobreak@eqnindex}{1}
\setcounter{@autobreak@subeqnindex}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{nlinenum}{0}
\setcounter{r@tfl@t}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{81}
\setcounter{maxnames}{99}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{2}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{cbx@tempcnta}{0}
\setcounter{cbx@tempcntb}{41}
\setcounter{cbx@tempcntc}{0}
\setcounter{cbx@tempcntd}{-1}
\setcounter{linenumber}{0}
\setcounter{LN@truepage}{32}
\setcounter{AlgoLine}{0}
\setcounter{algocfline}{0}
\setcounter{algocfproc}{0}
\setcounter{algocf}{0}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{float@type}{4}
\setcounter{su@anzahl}{0}
\setcounter{section@level}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{17}
\setcounter{SC@C}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{AM@survey}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{lemma}{0}
\setcounter{theorem}{0}
\setcounter{remark}{0}
}
